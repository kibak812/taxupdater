"""
ì›¹ í™˜ê²½ìš© ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ë“¤
tkinter ì˜ì¡´ì„± ì—†ì´ ì‘ë™í•˜ëŠ” í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import sys
import os

# ìƒìœ„ ë””ë ‰í† ë¦¬ ì„¤ì • import
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.config.settings import (
    URLS, SECTIONS, CRAWLING_CONFIG, 
    BAI_CLAIM_TYPES, SELENIUM_OPTIONS
)

# ì›¹ í™˜ê²½ìš© ê°€ì§œ progress/status í´ë˜ìŠ¤
class WebProgress:
    def __init__(self):
        self.value = 0
    
    def update(self):
        pass

class WebStatus:
    def __init__(self):
        self.text = ""
    
    def config(self, text):
        self.text = text
        print(f"[í¬ë¡¤ë§ ìƒíƒœ] {text}")
    
    def update(self):
        pass

# ì¬ì‹œë„ ë¡œì§
def safe_request(url, params, retries=None, delay=None):
    if retries is None:
        retries = CRAWLING_CONFIG["retry_count"]
    if delay is None:
        delay = CRAWLING_CONFIG["retry_delay"]
    
    for attempt in range(retries):
        try:
            response = requests.get(url, params=params, timeout=CRAWLING_CONFIG["timeout"])
            response.raise_for_status()
            return response
        except (requests.exceptions.RequestException, TimeoutError) as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt < retries - 1:
                print(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                print("Max retries reached. Exiting.")
                return None


def crawl_moef_site(progress=None, status_message=None, **kwargs):
    """ê¸°íšì¬ì •ë¶€ í¬ë¡¤ë§ (example.py ê¸°ë°˜)"""
    print("ğŸ” ê¸°íšì¬ì •ë¶€ í¬ë¡¤ë§ ì‹œì‘...")
    
    if progress is None:
        progress = WebProgress()
    if status_message is None:
        status_message = WebStatus()
    
    status_message.config("ê¸°íšì¬ì •ë¶€ ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
    
    # example.pyì˜ crawl_moef_site ë¡œì§ ì‚¬ìš©
    max_pages = kwargs.get('max_pages', 10)
    moef_url_template = "https://www.moef.go.kr/lw/intrprt/TaxLawIntrPrtCaseList.do?bbsId=MOSFBBS_000000000237&menuNo=8120300&pageIndex={page}"
    all_items = []
    total_items = 0
    items_per_page = 10
    
    # ì˜ˆìƒ ì´ í•­ëª© ìˆ˜ ê³„ì‚°
    total_estimated_items = items_per_page * max_pages
    
    for page in range(1, max_pages + 1):
        if status_message:
            status_message.config(f"ê¸°íšì¬ì •ë¶€ í˜ì´ì§€ {page}/{max_pages} í¬ë¡¤ë§ ì¤‘...")
        
        current_url = moef_url_template.format(page=page)
        response = safe_request(current_url, params={}, retries=3, delay=2)
        
        if not response:
            print(f"Failed to fetch page {page} from MOEF site. Skipping.")
            continue
            
        soup = BeautifulSoup(response.text, 'html.parser')
        items = soup.select("ul.boardType3.explnList > li")
        
        if not items:
            print(f"No items found on page {page}. This might be the last page.")
            break
            
        for item in items:
            title_element = item.select_one("h3 > a")
            doc_num_element = item.select_one("span.depart")
            date_element = item.select_one("span.date")
            
            if not all([title_element, doc_num_element, date_element]):
                continue
                
            title = title_element.get_text(strip=True)
            doc_num = doc_num_element.get_text(strip=True)
            response_date = date_element.get_text(strip=True).replace("íšŒì‹ ì¼ì :", "").strip()
            
            # ë§í¬ ì¶”ì¶œ - onclick ì†ì„±ì—ì„œ fn_egov_select('MOSF_ID') í˜•íƒœë¡œ ID ì¶”ì¶œ
            link = ""
            onclick_attr = title_element.get('onclick')
            if onclick_attr and "fn_egov_select" in onclick_attr:
                # fn_egov_select('MOSF_000000000073953') ì—ì„œ MOSF_000000000073953 ì¶”ì¶œ
                import re
                select_match = re.search(r"fn_egov_select\(['\"]([^'\"]+)['\"]\)", onclick_attr)
                if select_match:
                    mosf_id = select_match.group(1)
                    link = f"https://www.moef.go.kr/lw/intrprt/TaxLawIntrPrtCaseView.do?bbsId=MOSFBBS_000000000237&searchNttId1={mosf_id}&menuNo=8120300"
            
            all_items.append({
                "ë¬¸ì„œë²ˆí˜¸": doc_num,
                "íšŒì‹ ì¼ì": response_date,
                "ì œëª©": title,
                "ë§í¬": link
            })
        
        total_items += len(items)
        
        # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
        if progress:
            progress.value = int((total_items / total_estimated_items) * 100)
            progress.update()
    
    if status_message:
        status_message.config(f"ê¸°íšì¬ì •ë¶€ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_items}ê°œ í•­ëª© ìˆ˜ì§‘")
        
    return pd.DataFrame(all_items)

def crawl_mois_site(progress=None, status_message=None, **kwargs):
    """í–‰ì •ì•ˆì „ë¶€ í¬ë¡¤ë§ (example.py ê¸°ë°˜)"""
    print("ğŸ” í–‰ì •ì•ˆì „ë¶€ í¬ë¡¤ë§ ì‹œì‘...")
    
    if progress is None:
        progress = WebProgress()
    if status_message is None:
        status_message = WebStatus()
    
    # example.pyì˜ crawl_mois_site ë¡œì§ ì‚¬ìš©
    max_pages = kwargs.get('max_pages', 5)
    mois_url = "https://www.olta.re.kr/explainInfo/authoInterpretationList.do?menuNo=9020000&upperMenuId=9000000"
    
    options = Options()
    for option in SELENIUM_OPTIONS:
        options.add_argument(option)
    driver = webdriver.Chrome(options=options)
    
    try:
        driver.get(mois_url)
        
        if status_message:
            status_message.config("í–‰ì •ì•ˆì „ë¶€ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        time.sleep(10)  # JavaScript ì‹¤í–‰ì„ ìœ„í•œ ë” ê¸´ ëŒ€ê¸°ì‹œê°„
        
        all_items = []
        page = 1
        
        while page <= max_pages:
            try:
                if status_message:
                    status_message.config(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ {page}/{max_pages} í¬ë¡¤ë§ ì¤‘...")
                
                # ì²« í˜ì´ì§€ì—ì„œ HTML êµ¬ì¡° ë¶„ì„
                if page == 1:
                    print("í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ HTML êµ¬ì¡° ë¶„ì„ ì¤‘...")
                    page_source = driver.page_source[:3000]  # ì²˜ìŒ 3000ìë§Œ ì¶œë ¥
                    print("í˜ì´ì§€ ì†ŒìŠ¤ ìƒ˜í”Œ:")
                    print(page_source)
                
                # ì •í™•í•œ ì„ íƒìë¡œ ë¦¬ìŠ¤íŠ¸ í•­ëª© ì°¾ê¸°
                items = driver.find_elements(By.CSS_SELECTOR, "ul.search_out.exp li")
                
                if not items:
                    print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ {page}: 'ul.search_out.exp li' ì„ íƒìë¡œ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                
                page_items_count = 0
                for item in items:
                    try:
                        # HTML êµ¬ì¡°ì— ë”°ë¥¸ ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ
                        first_p = item.find_element(By.TAG_NAME, "p")
                        
                        # ì„¸ëª© ì¶”ì¶œ (span.part)
                        try:
                            tax_span = first_p.find_element(By.CSS_SELECTOR, "span.part")
                            tax_category = tax_span.text.strip()
                        except:
                            tax_category = ""
                        
                        # ì²« ë²ˆì§¸ p íƒœê·¸ì˜ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì„œë²ˆí˜¸ì™€ ë‚ ì§œ ì¶”ì¶œ
                        first_p_text = first_p.text.strip()
                        
                        # ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                        import re
                        date_pattern = r'\((\d{4}\.\d{2}\.\d{2})\)'
                        date_match = re.search(date_pattern, first_p_text)
                        
                        if date_match:
                            date_part = date_match.group(1)
                            # ì„¸ëª© ì´í›„ë¶€í„° ë‚ ì§œ ì´ì „ê¹Œì§€ê°€ ë¬¸ì„œë²ˆí˜¸
                            before_date = first_p_text[:date_match.start()].strip()
                            # ì„¸ëª©ì„ ì œê±°í•˜ê³  ë¬¸ì„œë²ˆí˜¸ë§Œ ì¶”ì¶œ
                            if tax_category:
                                doc_number = before_date.replace(tax_category, "").strip()
                            else:
                                doc_number = before_date
                        else:
                            date_part = ""
                            doc_number = ""
                        
                        # ì œëª© ë° ë§í¬ ì¶”ì¶œ (p.tt a)
                        try:
                            title_element = item.find_element(By.CSS_SELECTOR, "p.tt a")
                            title = title_element.text.strip()
                            
                            # ë§í¬ ì¶”ì¶œ - onclick ì†ì„±ì—ì„œ authoritativePopUp(ë²ˆí˜¸) í˜•íƒœë¡œ ë²ˆí˜¸ ì¶”ì¶œ
                            link = ""
                            onclick_attr = title_element.get_attribute("onclick")
                            if onclick_attr and "authoritativePopUp" in onclick_attr:
                                popup_match = re.search(r'authoritativePopUp\((\d+)\)', onclick_attr)
                                if popup_match:
                                    doc_id = popup_match.group(1)
                                    link = f"https://www.olta.re.kr/explainInfo/authoInterpretationDetail.do?num={doc_id}"
                        except:
                            title = ""
                            link = ""
                        
                        # ìœ íš¨í•œ ë°ì´í„°ì¸ì§€ í™•ì¸
                        if doc_number and title and date_part:
                            all_items.append({
                                "ì„¸ëª©": tax_category,
                                "ë¬¸ì„œë²ˆí˜¸": doc_number,
                                "ìƒì‚°ì¼ì": date_part,
                                "ì œëª©": title,
                                "ë§í¬": link
                            })
                            page_items_count += 1
                            
                    except Exception as e:
                        print(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ {page}: {page_items_count}ê°œ í•­ëª© ì¶”ì¶œ")
                
                if page_items_count == 0:
                    print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ, ì¤‘ë‹¨")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if page < max_pages:
                    try:
                        # JavaScript doPaging í•¨ìˆ˜ í˜¸ì¶œ
                        page_param = page * 10  # í˜ì´ì§€ ë²ˆí˜¸ëŠ” 10ë‹¨ìœ„ë¡œ ì¦ê°€ (10, 20, 30...)
                        print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ ì´ë™: doPaging('{page_param}')")
                        driver.execute_script(f"doPaging('{page_param}')")
                        time.sleep(3)
                        page += 1
                    except Exception as e:
                        print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                        break
                else:
                    break
                
                if progress:
                    progress.value = int((page / max_pages) * 100)
                    progress.update()
                    
            except Exception as e:
                print(f"í–‰ì •ì•ˆì „ë¶€ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                break
        
        if status_message:
            status_message.config(f"í–‰ì •ì•ˆì „ë¶€ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_items)}ê°œ í•­ëª© ìˆ˜ì§‘")
        
        print(f"í–‰ì •ì•ˆì „ë¶€: ì´ {len(all_items)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ")
        
        return pd.DataFrame(all_items, columns=["ì„¸ëª©", "ë¬¸ì„œë²ˆí˜¸", "ìƒì‚°ì¼ì", "ì œëª©", "ë§í¬"])

    finally:
        driver.quit()

def crawl_bai_site(progress=None, status_message=None, **kwargs):
    """ê°ì‚¬ì› í¬ë¡¤ë§ (example.py ê¸°ë°˜)"""
    print("ğŸ” ê°ì‚¬ì› í¬ë¡¤ë§ ì‹œì‘...")
    
    if progress is None:
        progress = WebProgress()
    if status_message is None:
        status_message = WebStatus()
    
    # example.pyì˜ crawl_bai_site ë¡œì§ ì‚¬ìš©
    max_pages = kwargs.get('max_pages', 10)
    bai_url = "https://www.bai.go.kr/bai/exClaims/exClaims/list/"
    
    options = Options()
    for option in SELENIUM_OPTIONS:
        options.add_argument(option)
    driver = webdriver.Chrome(options=options)
    
    # ì²­êµ¬ë¶„ì•¼ ëª©ë¡ (ì¡°ì„¸ ê´€ë ¨ë§Œ)
    claim_types = [
        {"name": "êµ­ì„¸", "value": "0102"},
        {"name": "ì§€ë°©ì„¸", "value": "0103"}
    ]
    
    try:
        all_data = []
        
        for claim_type in claim_types:
            if status_message:
                status_message.config(f"ê°ì‚¬ì› {claim_type['name']} ë°ì´í„° ë¡œë”© ì¤‘...")
            
            print(f"ê°ì‚¬ì› {claim_type['name']} ë¶„ì•¼ í¬ë¡¤ë§ ì‹œì‘")
            
            driver.get(bai_url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            try:
                WebDriverWait(driver, 15).until(
                    lambda driver: driver.find_elements(By.CSS_SELECTOR, "select#reqDvsnCd") or
                                   driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
                )
            except:
                print("ê°ì‚¬ì› ì‚¬ì´íŠ¸ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ëŒ€ê¸° í›„ ì§„í–‰")
                time.sleep(5)
            
            # ì²­êµ¬ë¶„ì•¼ë³„ URL íŒŒë¼ë¯¸í„°ë¡œ ì§ì ‘ ì ‘ê·¼
            try:
                search_url = f"{bai_url}?reqDvsnCd={claim_type['value']}"
                driver.get(search_url)
                print(f"ì²­êµ¬ë¶„ì•¼ '{claim_type['name']}' URLë¡œ ì§ì ‘ ì´ë™: {search_url}")
                time.sleep(5)
                
                # ë“œë¡­ë‹¤ìš´ ì„ íƒ ë°©ì‹ë„ ì‹œë„
                try:
                    select_element = driver.find_element(By.CSS_SELECTOR, "select#reqDvsnCd")
                    from selenium.webdriver.support.ui import Select
                    select = Select(select_element)
                    select.select_by_value(claim_type["value"])
                    print(f"ì²­êµ¬ë¶„ì•¼ '{claim_type['name']}' ë“œë¡­ë‹¤ìš´ ì„ íƒ ì™„ë£Œ")
                    time.sleep(1)
                    
                    # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
                    try:
                        search_button = driver.find_element(By.CSS_SELECTOR, "div.searchForm button[type='button']")
                        driver.execute_script("arguments[0].click();", search_button)
                        print("searchForm ë‚´ë¶€ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
                        time.sleep(3)
                    except Exception as e:
                        print(f"searchForm ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                    
                except Exception as dropdown_error:
                    print(f"ë“œë¡­ë‹¤ìš´ ì„ íƒ ì‹¤íŒ¨: {dropdown_error}, URL íŒŒë¼ë¯¸í„° ë°©ì‹ìœ¼ë¡œ ì§„í–‰")
                
            except Exception as e:
                print(f"ì²­êµ¬ë¶„ì•¼ '{claim_type['name']}' ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                continue
            
            # í•´ë‹¹ ë¶„ì•¼ì—ì„œ í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘
            page = 1
            pages_per_type = 10  # ê° ë¶„ì•¼ë³„ë¡œ 10í˜ì´ì§€ì”©
            
            while page <= pages_per_type:
                try:
                    if status_message:
                        status_message.config(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ {page}/{pages_per_type} í¬ë¡¤ë§ ì¤‘...")
                    
                    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í…Œì´ë¸” ì„ íƒì ì‹œë„
                    items = []
                    possible_selectors = [
                        "table tbody tr",
                        "table.board-list tbody tr", 
                        "table.tbl-list tbody tr",
                        ".board-list tbody tr",
                        "tbody tr"
                    ]
                    
                    for selector in possible_selectors:
                        items = driver.find_elements(By.CSS_SELECTOR, selector)
                        if items:
                            print(f"ê°ì‚¬ì›: '{selector}' ì„ íƒìë¡œ {len(items)}ê°œ í•­ëª© ë°œê²¬")
                            break
                    
                    if not items:
                        print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ {page}: ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        break
                    
                    page_data_count = 0
                    for item in items:
                        try:
                            cells = item.find_elements(By.TAG_NAME, "td")
                            if len(cells) >= 4:  # ìµœì†Œ 4ê°œ ì—´ í•„ìš”
                                # ì •í™•í•œ í…Œì´ë¸” êµ¬ì¡°ì— ë”°ë¥¸ ë°ì´í„° ì¶”ì¶œ
                                doc_number = cells[0].text.strip()      # ê²°ì •ë²ˆí˜¸
                                decision_date = cells[1].text.strip()   # ê²°ì •ì¼ì
                                category = cells[2].text.strip()        # êµ¬ë¶„
                                
                                # ì œëª© ì¶”ì¶œ (p.answerëŠ” ì œì™¸í•˜ê³  ì œëª©ë§Œ)
                                title_cell = cells[3]
                                try:
                                    # p.answer ìš”ì†Œê°€ ìˆë‹¤ë©´ ì œì™¸í•˜ê³  ì œëª©ë§Œ ì¶”ì¶œ
                                    answer_p = title_cell.find_element(By.CSS_SELECTOR, "p.answer")
                                    full_text = title_cell.text.strip()
                                    answer_text = answer_p.text.strip()
                                    title = full_text.replace(answer_text, "").strip()
                                except:
                                    # p.answerê°€ ì—†ë‹¤ë©´ ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì œëª©
                                    title = title_cell.text.strip()
                                
                                # ê²°ì •ë²ˆí˜¸ í˜•ì‹ í™•ì¸ (2025-ì‹¬ì‚¬-271 ê°™ì€ í˜•ì‹)
                                if doc_number and title and ("-" in doc_number and ("ì‹¬ì‚¬" in doc_number or "ê²°ì •" in doc_number)):
                                    all_data.append({
                                        "ì²­êµ¬ë¶„ì•¼": claim_type['name'],
                                        "ë¬¸ì„œë²ˆí˜¸": doc_number,
                                        "ê²°ì •ì¼ì": decision_date,
                                        "ì œëª©": title
                                    })
                                    page_data_count += 1
                                    
                        except Exception as e:
                            if page == 1:
                                print(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            continue
                    
                    print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ {page}: {page_data_count}ê°œ í•­ëª© ìˆ˜ì§‘")
                    
                    if page_data_count == 0:
                        print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ, ë‹¤ìŒ í˜ì´ì§€ ì‹œë„")
                    
                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                    if page < pages_per_type:
                        try:
                            next_page_num = page + 1
                            print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ ì´ë™: {next_page_num}í˜ì´ì§€")
                            page_link = driver.find_element(By.XPATH, f"//ul[@class='pages']//a[text()='{next_page_num}']")
                            page_link.click()
                            time.sleep(3)
                            page += 1
                        except Exception as e:
                            print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                            break
                    else:
                        break
                        
                except Exception as e:
                    print(f"ê°ì‚¬ì› {claim_type['name']} í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    break
            
            print(f"ê°ì‚¬ì› {claim_type['name']} ë¶„ì•¼ ì™„ë£Œ: {len([item for item in all_data if item.get('ì²­êµ¬ë¶„ì•¼') == claim_type['name']])}ê°œ í•­ëª© ìˆ˜ì§‘")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if progress:
                current_progress = (claim_types.index(claim_type) + 1) / len(claim_types) * 100
                progress.value = int(current_progress)
                progress.update()
        
        if status_message:
            status_message.config(f"ê°ì‚¬ì› í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_data)}ê°œ ì‚¬ë¡€ ìˆ˜ì§‘")
        
        print(f"ê°ì‚¬ì› ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ:")
        for claim_type in claim_types:
            type_count = len([item for item in all_data if item.get('ì²­êµ¬ë¶„ì•¼') == claim_type['name']])
            print(f"  {claim_type['name']}: {type_count}ê°œ")
        print(f"  ì „ì²´: {len(all_data)}ê°œ í•­ëª©")
        
        return pd.DataFrame(all_data, columns=["ì²­êµ¬ë¶„ì•¼", "ë¬¸ì„œë²ˆí˜¸", "ê²°ì •ì¼ì", "ì œëª©"])

    finally:
        driver.quit()

# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
if __name__ == "__main__":
    print("ì›¹ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸...")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    result = crawl_moef_site()
    print(f"ê¸°ì¬ë¶€ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(result)}ê°œ")
    print(result.head() if not result.empty else "ë°ì´í„° ì—†ìŒ")