"""
Tax Law Crawler Web Application - FastAPI ê¸°ë°˜ ì›¹ ì„œë²„

ê¸°ì¡´ SQLite ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œì„ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ ì œê³µ
í•µì‹¬ ëª©ì : ëˆ„ë½ì—†ëŠ” ë°ì´í„° ìˆ˜ì§‘ê³¼ ìƒˆë¡œìš´ ë°ì´í„° ì‹ë³„ì˜ ì›¹ ê¸°ë°˜ ëª¨ë‹ˆí„°ë§
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
from typing import Dict, Any, List
import asyncio
import json
from datetime import datetime
import uvicorn

# ê¸°ì¡´ í¬ë¡¤ë§ ì‹œìŠ¤í…œ import
from src.services.crawler_service import CrawlingService
from src.repositories.sqlite_repository import SQLiteRepository
from src.crawlers.tax_tribunal_crawler import TaxTribunalCrawler
from src.crawlers.nts_authority_crawler import NTSAuthorityCrawler
from src.crawlers.nts_precedent_crawler import NTSPrecedentCrawler
from src.config.settings import GUI_CONFIG

# ì›¹ í™˜ê²½ìš© ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ë“¤ import (tkinter ì˜ì¡´ì„± ì—†ìŒ)
try:
    from src.crawlers.web_legacy_crawlers import (
        crawl_moef_site, crawl_mois_site, crawl_bai_site
    )
    LEGACY_CRAWLERS_AVAILABLE = True
    print("âœ… ì›¹ í™˜ê²½ìš© ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ ì›¹ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ import ì‹¤íŒ¨: {e}")
    LEGACY_CRAWLERS_AVAILABLE = False
    
    # ë”ë¯¸ í•¨ìˆ˜ë“¤ë¡œ ëŒ€ì²´
    def crawl_moef_site(**kwargs):
        raise NotImplementedError("ì›¹ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì‚¬ìš© ë¶ˆê°€")
        
    def crawl_mois_site(**kwargs):
        raise NotImplementedError("ì›¹ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì‚¬ìš© ë¶ˆê°€")
        
    def crawl_bai_site(**kwargs):
        raise NotImplementedError("ì›¹ ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì‚¬ìš© ë¶ˆê°€")

class LegacyCrawlerWrapper:
    """ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘"""
    def __init__(self, site_name, site_key, crawler_func, key_column):
        self.site_name = site_name
        self.site_key = site_key
        self.crawler_func = crawler_func
        self.key_column = key_column
    
    def get_site_name(self):
        return self.site_name
    
    def get_site_key(self):
        return self.site_key
    
    def get_key_column(self):
        return self.key_column
    
    def crawl(self, progress_callback=None, status_callback=None, **kwargs):
        return self.crawler_func(progress=progress_callback, status_message=status_callback, **kwargs)
    
    def validate_data(self, data):
        return not data.empty if data is not None else False


# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="Tax Law Crawler Web Interface",
    description="ì„¸ê¸ˆ ë²•ë ¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì›¹ ì¸í„°í˜ì´ìŠ¤ - ëˆ„ë½ì—†ëŠ” ë°ì´í„° ìˆ˜ì§‘ê³¼ ìƒˆë¡œìš´ ë°ì´í„° ì‹ë³„",
    version="1.0.0"
)

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿ ì„¤ì •
static_path = project_root / "src" / "web" / "static"
templates_path = project_root / "src" / "web" / "templates"

app.mount("/static", StaticFiles(directory=str(static_path)), name="static")
templates = Jinja2Templates(directory=str(templates_path))

# í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
repository = SQLiteRepository()

# ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ (ì›¹ ì„œë²„ ì‹œì‘ ì‹œ)
print("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ ì¤‘...")
repository.force_schema_update()

# ê¸°ë³¸ í¬ë¡¤ëŸ¬ (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
crawlers = {
    "tax_tribunal": TaxTribunalCrawler(),
    "nts_authority": NTSAuthorityCrawler(),
    "nts_precedent": NTSPrecedentCrawler(),
}

# ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ë“¤ (tkinter ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€)
if LEGACY_CRAWLERS_AVAILABLE:
    crawlers.update({
        "moef": LegacyCrawlerWrapper(
            "ê¸°íšì¬ì •ë¶€", "moef", crawl_moef_site, "ë¬¸ì„œë²ˆí˜¸"
        ),
        "mois": LegacyCrawlerWrapper(
            "í–‰ì •ì•ˆì „ë¶€", "mois", crawl_mois_site, "ë¬¸ì„œë²ˆí˜¸"
        ),
        "bai": LegacyCrawlerWrapper(
            "ê°ì‚¬ì›", "bai", crawl_bai_site, "ë¬¸ì„œë²ˆí˜¸"
        )
    })
    print(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ì‚¬ìš© ê°€ëŠ¥: {len(crawlers)}ê°œ")
else:
    print(f"âš ï¸  ê¸°ë³¸ í¬ë¡¤ëŸ¬ë§Œ ì‚¬ìš© ê°€ëŠ¥: {len(crawlers)}ê°œ (ë ˆê±°ì‹œ í¬ë¡¤ëŸ¬ ì œì™¸)")

crawling_service = CrawlingService(crawlers, repository)

# WebSocket ì—°ê²° ê´€ë¦¬
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: dict, websocket: WebSocket):
        await websocket.send_text(json.dumps(message))

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_text(json.dumps(message))
            except:
                # ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ë¬´ì‹œ
                pass

manager = ConnectionManager()

# ì‚¬ì´íŠ¸ ì •ë³´ ë§¤í•‘ (ë™ì ìœ¼ë¡œ ìƒì„±)
BASE_SITE_INFO = {
    "tax_tribunal": {"name": "ì¡°ì„¸ì‹¬íŒì›", "color": "#3B82F6"},
    "nts_authority": {"name": "êµ­ì„¸ì²­", "color": "#10B981"},
    "moef": {"name": "ê¸°íšì¬ì •ë¶€", "color": "#8B5CF6"},
    "nts_precedent": {"name": "êµ­ì„¸ì²­ íŒë¡€", "color": "#F59E0B"},
    "mois": {"name": "í–‰ì •ì•ˆì „ë¶€", "color": "#EF4444"},
    "bai": {"name": "ê°ì‚¬ì›", "color": "#6B7280"}
}

# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ë§Œ í¬í•¨
SITE_INFO = {key: value for key, value in BASE_SITE_INFO.items() if key in crawlers}

@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/sites/{site_key}", response_class=HTMLResponse)
async def site_detail(request: Request, site_key: str):
    """ì‚¬ì´íŠ¸ë³„ ìƒì„¸ í˜ì´ì§€"""
    if site_key not in SITE_INFO:
        raise HTTPException(status_code=404, detail="Site not found")
    
    site_info = SITE_INFO[site_key]
    return templates.TemplateResponse("site.html", {
        "request": request,
        "site_key": site_key,
        "site_name": site_info["name"]
    })

@app.get("/api/dashboard")
async def get_dashboard_data():
    """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
    try:
        dashboard_data = []
        
        for site_key, site_info in SITE_INFO.items():
            stats = repository.get_statistics(site_key)
            
            dashboard_data.append({
                "site_key": site_key,
                "site_name": site_info["name"],
                "color": site_info["color"],
                "total_count": stats.get("total_count", 0),
                "last_updated": stats.get("last_updated"),
                "status": "success" if stats.get("total_count", 0) > 0 else "empty"
            })
        
        return {
            "sites": dashboard_data,
            "last_refresh": datetime.now().isoformat()
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Dashboard data error: {str(e)}")

@app.get("/api/sites/{site_key}/data")
async def get_site_data(site_key: str, page: int = 1, limit: int = 50, search: str = ""):
    """ì‚¬ì´íŠ¸ë³„ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜)"""
    try:
        if site_key not in SITE_INFO:
            raise HTTPException(status_code=404, detail="Site not found")
        
        # ì „ì²´ ë°ì´í„° ë¡œë“œ
        data = repository.load_existing_data(site_key)
        
        # ê²€ìƒ‰ í•„í„°ë§
        if search:
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì—ì„œ ê²€ìƒ‰
            text_columns = data.select_dtypes(include=['object']).columns
            mask = data[text_columns].astype(str).apply(
                lambda x: x.str.contains(search, case=False, na=False)
            ).any(axis=1)
            data = data[mask]
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        total_count = len(data)
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        page_data = data.iloc[start_idx:end_idx]
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        records = page_data.to_dict('records')
        
        return {
            "site_key": site_key,
            "site_name": SITE_INFO[site_key]["name"],
            "data": records,
            "pagination": {
                "page": page,
                "limit": limit,
                "total_count": total_count,
                "total_pages": (total_count + limit - 1) // limit,
                "has_next": end_idx < total_count,
                "has_prev": page > 1
            },
            "search": search
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Site data error: {str(e)}")

@app.post("/api/crawl/{site_key}")
async def start_crawling(site_key: str):
    """ê°œë³„ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘"""
    try:
        if site_key not in SITE_INFO:
            raise HTTPException(status_code=404, detail="Site not found")
        
        # site_keyë¥¼ choiceë¡œ ë³€í™˜í•˜ëŠ” ë§¤í•‘
        site_to_choice_mapping = {
            "tax_tribunal": "1",
            "nts_authority": "2", 
            "moef": "3",
            "nts_precedent": "4",
            "mois": "5",
            "bai": "6"
        }
        
        choice = site_to_choice_mapping.get(site_key)
        if not choice:
            raise HTTPException(status_code=400, detail=f"Invalid site_key: {site_key}")
        
        # ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        asyncio.create_task(run_crawling_task(choice))
        
        return {
            "status": "started",
            "site_key": site_key,
            "site_name": SITE_INFO[site_key]["name"],
            "message": f"{SITE_INFO[site_key]['name']} í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Crawling start error: {str(e)}")

@app.post("/api/crawl/all")
async def start_all_crawling():
    """ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘"""
    try:
        # ë¹„ë™ê¸°ì ìœ¼ë¡œ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        asyncio.create_task(run_crawling_task("7"))  # choice = "7"ì€ ì „ì²´ í¬ë¡¤ë§
        
        return {
            "status": "started",
            "message": "ì „ì²´ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"All crawling start error: {str(e)}")

@app.get("/api/stats")
async def get_statistics():
    """ì „ì²´ í†µê³„ ì •ë³´"""
    try:
        total_records = 0
        site_stats = {}
        
        for site_key, site_info in SITE_INFO.items():
            stats = repository.get_statistics(site_key)
            site_count = stats.get("total_count", 0)
            total_records += site_count
            
            site_stats[site_key] = {
                "name": site_info["name"],
                "count": site_count,
                "last_updated": stats.get("last_updated")
            }
        
        return {
            "total_records": total_records,
            "site_stats": site_stats,
            "database_info": repository.get_database_info()
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Statistics error: {str(e)}")

async def run_crawling_task(choice: str):
    """í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰ (ë¹„ë™ê¸°)"""
    try:
        await manager.broadcast({
            "type": "crawl_start",
            "choice": choice,
            "timestamp": datetime.now().isoformat()
        })
        
        # ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ì‹¤í–‰
        import asyncio
        import concurrent.futures
        
        # WebSocket ì—°ë™ ì§„í–‰ìƒí™© ì½œë°± (Thread-safe ì‹¤ì‹œê°„ ì›¹ í”¼ë“œë°±)
        class WebSocketProgress:
            def __init__(self, loop, manager, choice):
                self.value = 0
                self.loop = loop
                self.manager = manager
                self.choice = choice
            
            def update(self):
                # Thread-safe WebSocket ì „ì†¡
                try:
                    message = {
                        "type": "crawl_progress",
                        "choice": self.choice,
                        "progress": self.value,
                        "timestamp": datetime.now().isoformat()
                    }
                    # asyncio.run_coroutine_threadsafeë¡œ ë¹„ë™ê¸° ì‘ì—… ìŠ¤ì¼€ì¤„ë§
                    asyncio.run_coroutine_threadsafe(
                        self.manager.broadcast(message), 
                        self.loop
                    )
                except Exception as e:
                    print(f"ì§„í–‰ë¥  WebSocket ì „ì†¡ ì˜¤ë¥˜: {e}")
        
        class WebSocketStatus:
            def __init__(self, loop, manager, choice):
                self.text = ""
                self.loop = loop
                self.manager = manager
                self.choice = choice
            
            def config(self, text):
                self.text = text
                print(f"[í¬ë¡¤ë§ ìƒíƒœ] {text}")
                # Thread-safe WebSocket ì „ì†¡
                try:
                    message = {
                        "type": "crawl_status",
                        "choice": self.choice,
                        "status": text,
                        "timestamp": datetime.now().isoformat()
                    }
                    # asyncio.run_coroutine_threadsafeë¡œ ë¹„ë™ê¸° ì‘ì—… ìŠ¤ì¼€ì¤„ë§
                    asyncio.run_coroutine_threadsafe(
                        self.manager.broadcast(message), 
                        self.loop
                    )
                except Exception as e:
                    print(f"ìƒíƒœ WebSocket ì „ì†¡ ì˜¤ë¥˜: {e}")
            
            def update(self):
                # ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œì—ë„ ì „ì†¡
                if self.text:
                    self.config(self.text)
        
        def run_sync_crawling():
            """ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰ (Thread-safe WebSocket ì—°ë™)"""
            try:
                # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê°€ì ¸ì™€ì„œ WebSocket ì—°ë™ ì§„í–‰ìƒí™© ê°ì²´ ìƒì„±
                progress = WebSocketProgress(loop, manager, choice)
                status = WebSocketStatus(loop, manager, choice)
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                crawling_service.execute_crawling(choice, progress, status, is_periodic=False)
                return {"status": "success", "message": "í¬ë¡¤ë§ ì™„ë£Œ"}
                
            except Exception as e:
                print(f"í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return {"status": "error", "error": str(e)}
        
        # ThreadPoolExecutorë¡œ ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸° ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            result = await loop.run_in_executor(executor, run_sync_crawling)
        
        # ê²°ê³¼ì— ë”°ë¥¸ ì•Œë¦¼
        if result["status"] == "success":
            await manager.broadcast({
                "type": "crawl_complete",
                "choice": choice,
                "timestamp": datetime.now().isoformat()
            })
        else:
            await manager.broadcast({
                "type": "crawl_error",
                "error": result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"),
                "timestamp": datetime.now().isoformat()
            })
        
    except Exception as e:
        await manager.broadcast({
            "type": "crawl_error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        })

@app.websocket("/ws/crawl")
async def websocket_endpoint(websocket: WebSocket):
    """í¬ë¡¤ë§ ìƒíƒœ ì‹¤ì‹œê°„ WebSocket"""
    await manager.connect(websocket)
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ëŒ€ê¸° (ì—°ê²° ìœ ì§€ìš©)
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)

if __name__ == "__main__":
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )